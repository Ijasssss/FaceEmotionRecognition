{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44edbae0-9456-45c2-be9b-f6977226e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.image import load_img\n",
    "import PIL\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fef036f1-b9ed-4b0f-829f-eb9cfda95b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'images/train'\n",
    "TEST_DIR = 'images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43cf0e79-ecd0-41c4-8d98-1afc12d124be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdataframe(dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        label_path = os.path.join(dir, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "        for imagename in os.listdir(os.path.join(dir,label)):\n",
    "            image_paths.append(os.path.join(dir,label,imagename))\n",
    "            labels.append(label)\n",
    "        print(label, \"completed\")\n",
    "    return image_paths,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b757354e-459e-42b1-b8fd-a5a2d6d5862d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy completed\n",
      "sad completed\n",
      "fear completed\n",
      "surprise completed\n",
      "neutral completed\n",
      "angry completed\n",
      "disgust completed\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame()\n",
    "train['image'], train['label'] = createdataframe(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03e95fa1-6870-452b-b638-0e34b4f5da1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                image    label\n",
      "0         images/train/happy/3578.jpg    happy\n",
      "1        images/train/happy/16988.jpg    happy\n",
      "2         images/train/happy/2666.jpg    happy\n",
      "3         images/train/happy/5109.jpg    happy\n",
      "4        images/train/happy/11981.jpg    happy\n",
      "...                               ...      ...\n",
      "28816  images/train/disgust/10112.jpg  disgust\n",
      "28817  images/train/disgust/21668.jpg  disgust\n",
      "28818   images/train/disgust/7049.jpg  disgust\n",
      "28819   images/train/disgust/9716.jpg  disgust\n",
      "28820   images/train/disgust/3561.jpg  disgust\n",
      "\n",
      "[28821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dee2a5b8-547b-49cf-b512-6024ffcd2647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy completed\n",
      "sad completed\n",
      "fear completed\n",
      "surprise completed\n",
      "neutral completed\n",
      "angry completed\n",
      "disgust completed\n"
     ]
    }
   ],
   "source": [
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = createdataframe(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8df9f8d6-6414-4a75-8e60-67faeaa3ff85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              image    label\n",
      "0       images/test/happy/23933.jpg    happy\n",
      "1       images/test/happy/24906.jpg    happy\n",
      "2       images/test/happy/18033.jpg    happy\n",
      "3       images/test/happy/15271.jpg    happy\n",
      "4       images/test/happy/26888.jpg    happy\n",
      "...                             ...      ...\n",
      "7061  images/test/disgust/20761.jpg  disgust\n",
      "7062  images/test/disgust/28710.jpg  disgust\n",
      "7063  images/test/disgust/23876.jpg  disgust\n",
      "7064   images/test/disgust/9460.jpg  disgust\n",
      "7065  images/test/disgust/35580.jpg  disgust\n",
      "\n",
      "[7066 rows x 2 columns]\n",
      "0         images/test/happy/23933.jpg\n",
      "1         images/test/happy/24906.jpg\n",
      "2         images/test/happy/18033.jpg\n",
      "3         images/test/happy/15271.jpg\n",
      "4         images/test/happy/26888.jpg\n",
      "                    ...              \n",
      "7061    images/test/disgust/20761.jpg\n",
      "7062    images/test/disgust/28710.jpg\n",
      "7063    images/test/disgust/23876.jpg\n",
      "7064     images/test/disgust/9460.jpg\n",
      "7065    images/test/disgust/35580.jpg\n",
      "Name: image, Length: 7066, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "766567ba-6538-41de-bbb1-303e1762aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bbf31d4b-600b-444e-8e8b-0263d65f11bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image,grayscale = True)\n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(len(features),48, 48,1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03952439-4d63-4462-9563-4ecdea3a5692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75bda8e2afe47cb9528f5aa946c28ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import PIL.Image. The use of `load_img` requires PIL.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_features = \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mextract_features\u001b[39m\u001b[34m(images)\u001b[39m\n\u001b[32m      2\u001b[39m features = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m tqdm(images):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     img = \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgrayscale\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     img = np.array(img)\n\u001b[32m      6\u001b[39m     features.append(img)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FaceEmotionRecognition/.conda/lib/python3.12/site-packages/keras_preprocessing/image/utils.py:111\u001b[39m, in \u001b[36mload_img\u001b[39m\u001b[34m(path, grayscale, color_mode, target_size, interpolation)\u001b[39m\n\u001b[32m    109\u001b[39m     color_mode = \u001b[33m'\u001b[39m\u001b[33mgrayscale\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pil_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mCould not import PIL.Image. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    112\u001b[39m                       \u001b[33m'\u001b[39m\u001b[33mThe use of `load_img` requires PIL.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    114\u001b[39m     img = pil_image.open(io.BytesIO(f.read()))\n",
      "\u001b[31mImportError\u001b[39m: Could not import PIL.Image. The use of `load_img` requires PIL."
     ]
    }
   ],
   "source": [
    "train_features = extract_features(train['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12127053-40c7-4d17-a2d6-0affc7e0d29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86f6b871cf045ffb4beac40a00af2da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7066 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import PIL.Image. The use of `load_img` requires PIL.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m test_features = \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mextract_features\u001b[39m\u001b[34m(images)\u001b[39m\n\u001b[32m      2\u001b[39m features = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m tqdm(images):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     img = \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgrayscale\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     img = np.array(img)\n\u001b[32m      6\u001b[39m     features.append(img)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FaceEmotionRecognition/.conda/lib/python3.12/site-packages/keras_preprocessing/image/utils.py:111\u001b[39m, in \u001b[36mload_img\u001b[39m\u001b[34m(path, grayscale, color_mode, target_size, interpolation)\u001b[39m\n\u001b[32m    109\u001b[39m     color_mode = \u001b[33m'\u001b[39m\u001b[33mgrayscale\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pil_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mCould not import PIL.Image. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    112\u001b[39m                       \u001b[33m'\u001b[39m\u001b[33mThe use of `load_img` requires PIL.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    114\u001b[39m     img = pil_image.open(io.BytesIO(f.read()))\n",
      "\u001b[31mImportError\u001b[39m: Could not import PIL.Image. The use of `load_img` requires PIL."
     ]
    }
   ],
   "source": [
    "test_features = extract_features(test['image'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
